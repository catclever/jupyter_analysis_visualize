# Claude Code Development Guidelines

## Project Structure

### Backend Testing

All backend test scripts should be placed in the `backend/test/` directory.

**Guidelines:**
- Test files should follow the naming convention: `test_*.py`
- Each module should have corresponding test file
- Tests can be run individually or as a suite
- Generated test artifacts (`.ipynb`, `.parquet`, etc.) should be cleaned up after tests

**Current test files:**
- `backend/test/test_notebook_manager.py` - Tests for NotebookManager (âœ… Implemented)
- `backend/test/test_project_manager.py` - Tests for ProjectManager (ðŸ“‹ Planned)
- `backend/test/test_kernel_manager.py` - Tests for KernelManager (ðŸ“‹ Planned)
- `backend/test/test_execution_manager.py` - Tests for ExecutionManager (ðŸ“‹ Planned)

**Example:**
```bash
# Run a specific test
python backend/test/test_notebook_manager.py

# Run all tests
cd backend/test && python -m pytest
```

### Directory Structure

```
jupyter_analysis_visualize/
â”œâ”€â”€ frontend/                    # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/                     # Python backend
â”‚   â”œâ”€â”€ notebook_manager.py      # Core modules
â”‚   â”œâ”€â”€ project_manager.py       # (To be implemented)
â”‚   â”œâ”€â”€ kernel_manager.py        # (To be implemented)
â”‚   â”œâ”€â”€ execution_manager.py     # (To be implemented)
â”‚   â”œâ”€â”€ node_functions/          # Actual analysis functions (for different projects)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_analysis/       # Functions for 'data-analysis' project
â”‚   â”‚   â”‚   â”œâ”€â”€ data_nodes.py    # data_source nodes implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ compute_nodes.py # compute nodes implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ chart_nodes.py   # chart nodes implementation
â”‚   â”‚   â”‚   â””â”€â”€ tool_nodes.py    # tool nodes implementation
â”‚   â”‚   â””â”€â”€ risk_model/          # Functions for 'risk-model' project (example)
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ test/                    # Test directory
â”‚   â”‚   â”œâ”€â”€ test_notebook_manager.py
â”‚   â”‚   â”œâ”€â”€ test_kernel_manager.py        # (To be created)
â”‚   â”‚   â”œâ”€â”€ test_execution_manager.py     # (To be created)
â”‚   â”‚   â”œâ”€â”€ test_project_manager.py       # (To be created)
â”‚   â”‚   â””â”€â”€ README.md            # Test documentation
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ API_DESIGN.md
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ .gitignore                   # Updated to ignore backend/test outputs
â””â”€â”€ README.md
```

### Gitignore Rules

The `.gitignore` file includes rules to ignore:
- `backend/test/test_*.ipynb` - Generated test notebooks
- `backend/test/test_*.parquet` - Generated test parquets
- `backend/test/*.json` - Generated test artifacts
- All other backend/test/* temporary files are not tracked

**Why ignore test artifacts?**
- Test notebooks and data files are temporary and can be regenerated
- They bloat the git history
- Each developer may run tests with different data
- The test scripts themselves ARE tracked (not ignored)

## Implementation Notes

### NotebookManager (Completed)
- Handles incremental cell addition to .ipynb files
- Supports all 4 node types: data_source, compute, chart, tool
- Properly formats cell metadata for parsing
- No code validation (accepts any Python code as input)

### ProjectManager (In Progress)
- Manages project directory structure
- Handles project initialization
- Manages project.json configuration
- Manages parquets storage directory

### KernelManager (Planned)
- Jupyter kernel lifecycle management
- Per-project kernel instances
- Connection pooling
- Timeout handling

### ExecutionManager (Planned)
- Execute nodes with dependency resolution
- Auto-save results to parquet/json
- Error handling and recovery
- Breakpoint support

### ProjectMetadataParser (Planned)
- Parse .ipynb cells and extract node metadata
- Build DAG from dependencies
- Validate notebook structure
- (To be integrated with frontend for debugging)

### node_functions/ Directory
Contains the actual analysis function implementations organized by project:
- **Purpose**: Store Python functions that are executed by nodes
- **Organization**: One subdirectory per project (e.g., `data_analysis/`, `risk_model/`)
- **Contents**: Each project subdirectory has modules for different node types:
  - `data_nodes.py` - Functions for data_source nodes (loading, importing data)
  - `compute_nodes.py` - Functions for compute nodes (analysis, transformations)
  - `chart_nodes.py` - Functions for chart nodes (visualization with Plotly/PyEcharts)
  - `tool_nodes.py` - Functions for tool nodes (reusable toolkits)

**Example usage in notebook:**
```python
# In project.ipynb
# @node_type: data_source
# @node_id: data_1
from node_functions.data_analysis.data_nodes import load_user_data
data_1 = load_user_data()

# @node_type: compute
# @node_id: compute_1
from node_functions.data_analysis.compute_nodes import analyze_features
compute_1 = analyze_features(data_1, data_2)

# @node_type: chart
# @node_id: chart_1
from node_functions.data_analysis.chart_nodes import create_age_chart
chart_1 = create_age_chart(compute_1)

# @node_type: tool
# @node_id: tool_preprocessing
from node_functions.data_analysis.tool_nodes import tool_preprocessing
# tool_preprocessing is now available in kernel
```

## Development Workflow

1. **Create test file**: `backend/test/test_*.py`
2. **Implement module**: `backend/*.py`
3. **Run tests**: `python backend/test/test_*.py`
4. **Clean up artifacts**: Tests should clean up generated files
5. **Commit**: Only commit the module and test code, not artifacts

## Code Style

- Follow PEP 8
- Use type hints where possible
- Add docstrings for public methods
- Keep modules focused and single-responsibility

## Testing Best Practices

- Each test should be independent
- Use `setUp` and `tearDown` for cleanup
- Generate test artifacts in temporary locations when possible
- Test both success and failure cases
- Keep test output clear and descriptive
